{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8027091,"sourceType":"datasetVersion","datasetId":4730821},{"sourceId":8042029,"sourceType":"datasetVersion","datasetId":4741438}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# List all image files under the input directory and store them in a list\nimage_paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            full_path = os.path.join(dirname, filename)\n            image_paths.append(full_path)\n            print(full_path)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"execution":{"iopub.status.busy":"2024-04-07T10:52:45.265341Z","iopub.execute_input":"2024-04-07T10:52:45.265720Z","iopub.status.idle":"2024-04-07T10:52:45.339260Z","shell.execute_reply.started":"2024-04-07T10:52:45.265680Z","shell.execute_reply":"2024-04-07T10:52:45.338222Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"/kaggle/input/car-notcars/notcars/notcar10.jpg\n/kaggle/input/car-notcars/notcars/notcar7.jpg\n/kaggle/input/car-notcars/notcars/notcar2.jpg\n/kaggle/input/car-notcars/notcars/notcar4.jpg\n/kaggle/input/car-notcars/notcars/notcar9.jpg\n/kaggle/input/car-notcars/notcars/notcar8.jpg\n/kaggle/input/car-notcars/notcars/notcar1.jpg\n/kaggle/input/car-notcars/notcars/notcar5.jpg\n/kaggle/input/car-notcars/notcars/notcar3.jpg\n/kaggle/input/car-notcars/notcars/notcar6.jpg\n/kaggle/input/car-notcars/cars/car12.jpg\n/kaggle/input/car-notcars/cars/car1.jpg\n/kaggle/input/car-notcars/cars/car9.jpg\n/kaggle/input/car-notcars/cars/car4.jpg\n/kaggle/input/car-notcars/cars/car10.jpg\n/kaggle/input/car-notcars/cars/car5.jpg\n/kaggle/input/car-notcars/cars/car11.jpg\n/kaggle/input/car-notcars/cars/car13.jpg\n/kaggle/input/car-notcars/cars/car2.jpg\n/kaggle/input/car-notcars/cars/car3.jpg\n/kaggle/input/testcarsnoncars/nontestcar/testnotcar5.jpg\n/kaggle/input/testcarsnoncars/nontestcar/testnotcar3.jpg\n/kaggle/input/testcarsnoncars/nontestcar/testnotcar4.jpg\n/kaggle/input/testcarsnoncars/nontestcar/testnotcar1.jpg\n/kaggle/input/testcarsnoncars/nontestcar/testnotcar2.jpg\n/kaggle/input/testcarsnoncars/testcar/test5.jpg\n/kaggle/input/testcarsnoncars/testcar/testcar4.jpg\n/kaggle/input/testcarsnoncars/testcar/testcar1.png\n/kaggle/input/testcarsnoncars/testcar/testcar3.jpg\n/kaggle/input/testcarsnoncars/testcar/testcar2.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# Let's define the file paths for car and not-car images according to the uploaded image\ncar_image_paths = [\n    '/kaggle/input/car-notcars/cars/car1.jpg',\n    '/kaggle/input/car-notcars/cars/car2.jpg',\n    '/kaggle/input/car-notcars/cars/car3.jpg',\n    '/kaggle/input/car-notcars/cars/car4.jpg',\n    '/kaggle/input/car-notcars/cars/car5.jpg',\n    '/kaggle/input/car-notcars/cars/car9.jpg',\n    '/kaggle/input/car-notcars/cars/car10.jpg',\n    '/kaggle/input/car-notcars/cars/car11.jpg',\n    '/kaggle/input/car-notcars/cars/car12.jpg',\n    '/kaggle/input/car-notcars/cars/car13.jpg'\n]\n\nnot_car_image_paths = [\n    '/kaggle/input/car-notcars/notcars/notcar1.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar2.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar3.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar4.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar5.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar6.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar7.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar8.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar9.jpg',\n    '/kaggle/input/car-notcars/notcars/notcar10.jpg'\n]\n\n# Define the function to load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size=(64, 64)):\n    images = []\n    for path in image_paths:\n        # Read the image from the file path\n        img = imread(path)\n        \n        # Resize the image to the target size\n        img = resize(img, target_size, anti_aliasing=True)\n        \n        # If the image has an alpha channel, remove it\n        if img.shape[2] == 4:\n            img = img[..., :3]\n        \n        # Normalize pixel values to be between 0 and 1\n        img = img.astype(np.float32) / 255.0\n        \n        # Flatten the image\n        img_flatten = img.flatten()\n        \n        # Append the flattened image to the list\n        images.append(img_flatten)\n    \n    # Stack images into a matrix\n    image_matrix = np.vstack(images)\n    return image_matrix\n\n# Use the function to load car and not-car images\ncar_images = load_and_preprocess_images(car_image_paths)\nnot_car_images = load_and_preprocess_images(not_car_image_paths)\n\n# Verify the shapes of the loaded image matrices\ncar_images.shape, not_car_images.shape\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-04-07T10:52:45.341861Z","iopub.execute_input":"2024-04-07T10:52:45.342351Z","iopub.status.idle":"2024-04-07T10:52:45.658290Z","shell.execute_reply.started":"2024-04-07T10:52:45.342310Z","shell.execute_reply":"2024-04-07T10:52:45.657299Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"((10, 12288), (10, 12288))"},"metadata":{}}]},{"cell_type":"code","source":"image_data = load_and_preprocess_images(image_paths)\n","metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-04-07T10:52:45.659364Z","iopub.execute_input":"2024-04-07T10:52:45.659644Z","iopub.status.idle":"2024-04-07T10:52:46.041184Z","shell.execute_reply.started":"2024-04-07T10:52:45.659620Z","shell.execute_reply":"2024-04-07T10:52:46.040240Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"image_data","metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-04-07T10:52:46.043742Z","iopub.execute_input":"2024-04-07T10:52:46.044482Z","iopub.status.idle":"2024-04-07T10:52:46.051756Z","shell.execute_reply.started":"2024-04-07T10:52:46.044430Z","shell.execute_reply":"2024-04-07T10:52:46.050635Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"array([[1.0853694e-03, 1.2717748e-03, 1.3021096e-03, ..., 7.2799233e-04,\n        1.0899197e-03, 2.2446839e-04],\n       [1.8354665e-03, 1.7195963e-03, 1.6873118e-03, ..., 2.5620095e-03,\n        2.4025026e-03, 2.3840719e-03],\n       [8.9299493e-04, 1.2944702e-03, 1.3112804e-03, ..., 1.9529340e-05,\n        5.0435943e-04, 6.1294239e-04],\n       ...,\n       [3.6139947e-03, 2.5067283e-03, 3.6447521e-03, ..., 3.6139947e-03,\n        2.5067283e-03, 3.6447521e-03],\n       [2.3755138e-03, 1.8193581e-03, 1.3201239e-03, ..., 1.9556191e-03,\n        1.9556191e-03, 1.9248665e-03],\n       [3.9215689e-03, 3.9215689e-03, 3.9215689e-03, ..., 3.9215689e-03,\n        3.9215689e-03, 3.9215689e-03]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# We already have the preprocessed images in car_images and not_car_images.\n# Convert them to PyTorch tensors\ncar_images_tensor = torch.tensor(car_images, dtype=torch.float32)\nnot_car_images_tensor = torch.tensor(not_car_images, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:52:46.053272Z","iopub.execute_input":"2024-04-07T10:52:46.054133Z","iopub.status.idle":"2024-04-07T10:52:46.065011Z","shell.execute_reply.started":"2024-04-07T10:52:46.054097Z","shell.execute_reply":"2024-04-07T10:52:46.064070Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#Performing PCA \ndef pca_step_by_step(images_tensor):\n    # Standardize the images\n    mean = images_tensor.mean(dim=0)\n    std = images_tensor.std(dim=0) + 1e-5\n    standardized_images = (images_tensor - mean) / std\n    \n    # Compute the covariance matrix\n    covariance_matrix = torch.mm(standardized_images.t(), standardized_images) / (standardized_images.size(0) - 1)\n    \n    # Compute the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix)\n    \n    # Sort the eigenvalues in descending order, and sort the eigenvectors to match\n    sorted_indices = torch.argsort(eigenvalues, descending=True)\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the number of components that explain 95% of the variance\n    total_variance = sorted_eigenvalues.sum()\n    variance_ratios = sorted_eigenvalues.cumsum(0) / total_variance\n    n_components = (variance_ratios < 0.95).sum().item() + 1\n    \n    # Select the principal components (eigenvectors) - PCA subspace\n    principal_components = sorted_eigenvectors[:, :n_components]\n    \n    return principal_components, mean, std, n_components","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:52:46.066108Z","iopub.execute_input":"2024-04-07T10:52:46.066415Z","iopub.status.idle":"2024-04-07T10:52:46.076522Z","shell.execute_reply.started":"2024-04-07T10:52:46.066390Z","shell.execute_reply":"2024-04-07T10:52:46.075737Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Apply the step-by-step PCA to car images\nprincipal_components_car, mean_car, std_car, n_components_car = pca_step_by_step(car_images_tensor)\n\n# Apply the step-by-step PCA to not-car images\nprincipal_components_not_car, mean_not_car, std_not_car, n_components_not_car = pca_step_by_step(not_car_images_tensor)\n\n# Print the number of components and shapes of the principal components matrices\nprint(\"Car PCA components: \", n_components_car, principal_components_car.shape)\nprint(\"Not-Car PCA components: \", n_components_not_car, principal_components_not_car.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:52:46.077684Z","iopub.execute_input":"2024-04-07T10:52:46.078060Z","iopub.status.idle":"2024-04-07T10:56:05.093642Z","shell.execute_reply.started":"2024-04-07T10:52:46.078021Z","shell.execute_reply":"2024-04-07T10:56:05.092550Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Car PCA components:  8 torch.Size([12288, 8])\nNot-Car PCA components:  8 torch.Size([12288, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"#PCA subspace coefficients -> projecting the images on the PCA subspace (dimensionality reduction)\nprojected_images_car_on_car=torch.mm((car_images_tensor-mean_car)/std_car,principal_components_car)\nprojected_images_car_on_not_car=torch.mm((car_images_tensor-mean_car)/std_car,principal_components_not_car)\n\nprojected_images_not_car_on_not_car=torch.mm((not_car_images_tensor-mean_not_car)/std_not_car,principal_components_not_car)\nprojected_images_not_car_on_car=torch.mm((not_car_images_tensor-mean_not_car)/std_not_car,principal_components_car)\n\nprint(projected_images_car_on_car.shape,projected_images_car_on_not_car.shape)\nprint(projected_images_car_on_car,projected_images_car_on_not_car)\n\nprint(projected_images_not_car_on_not_car.shape,projected_images_not_car_on_car.shape)\nprint(projected_images_not_car_on_not_car,projected_images_not_car_on_car)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:05.095012Z","iopub.execute_input":"2024-04-07T10:56:05.095330Z","iopub.status.idle":"2024-04-07T10:56:05.113162Z","shell.execute_reply.started":"2024-04-07T10:56:05.095301Z","shell.execute_reply":"2024-04-07T10:56:05.112138Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"torch.Size([10, 8]) torch.Size([10, 8])\ntensor([[-92.3996,  21.8485,  -5.5246, -51.1915, -32.9772,  41.4920,   8.9848,\n          12.6923],\n        [ 58.7024, -83.3238,  18.8182, -52.6354,   4.9698, -11.9012,  10.3221,\n          -5.5889],\n        [-45.6675, -52.0576,  -9.2052,  41.3129, -17.1868,  -1.5775, -43.6493,\n          27.4001],\n        [ 30.7586,  50.2127,  -9.1391, -35.3531,  10.0584, -35.7483, -47.0419,\n          -3.1496],\n        [-33.4219,  -7.2585, -20.8230,  24.6934,  -8.6326, -38.6567,  40.7657,\n           5.6886],\n        [ 89.0806,   1.3363, -59.7714,  21.2838, -22.5094,  35.4665,  -1.1914,\n         -27.6370],\n        [-20.9126,  29.5792, -23.3323,   1.5863,  -1.8441, -35.1611,  19.8261,\n          -2.3110],\n        [-36.5417,  -1.2339, -14.4264,   7.5492,  79.6215,  27.9449,   3.9368,\n           1.1004],\n        [ 98.2989,  35.2882,  53.1458,  17.9211,  -2.7107,  15.4820,  14.2983,\n          34.8876],\n        [-47.8972,   5.6089,  70.2580,  24.8332,  -8.7888,   2.6594,  -6.2511,\n         -43.0824]]) tensor([[-10.5872,  35.8644,  -3.2522,   3.8497,  29.2410,  -6.9607,  -1.4694,\n         -10.1952],\n        [ 21.3436, -26.1052,  24.9259,  21.4419,  -6.8739,  14.0944,   0.8834,\n           6.0348],\n        [-36.3959,  21.8237,   0.9356,  24.7222,  12.2685, -11.6241, -10.8073,\n           6.3324],\n        [ 13.8780,  -6.4840,  -9.2042,  -3.1160, -16.8834,  13.3139,   0.9163,\n         -10.4689],\n        [-23.0422,  -3.4143,  -2.5408,   2.0633,  21.2493,  -2.8203,   1.2888,\n          15.8858],\n        [-11.5921, -36.6958, -13.3390, -35.8856, -14.2010,  19.6805,  12.6017,\n          -9.1356],\n        [-12.5821,   3.2304,  -6.2271,  -3.0142,   4.6986,   5.1723,   3.0239,\n           5.3487],\n        [-10.0009,  23.7640,  -1.0555,   6.3598,  18.5723,   1.1499,   5.4479,\n           3.1446],\n        [ 54.0765, -40.0597,  10.0508, -21.8153, -49.6211, -12.2404,   1.3114,\n          -4.0492],\n        [ 14.9022,  28.0764,  -0.2935,   5.3942,   1.5497, -19.7654, -13.1967,\n          -2.8973]])\ntorch.Size([10, 8]) torch.Size([10, 8])\ntensor([[ 17.9365,  -8.6157, -13.9354, -88.5838,  -3.7356,   5.3866,  12.0129,\n          14.2722],\n        [-69.3587, -35.9795,  30.6830,  26.9578,  29.1274,  30.8706,  29.9336,\n          15.4956],\n        [ 13.2407,  -7.0492,  24.4462,   5.9682, -67.8630,  30.1611,  -2.7268,\n         -29.7477],\n        [ 93.4410, -47.4920, -74.9572,  30.5021,   4.6631,   2.3630,   0.9791,\n           0.3280],\n        [131.3086,  29.8963,  56.9057,   6.3389,  26.9421,   4.8116, -15.5176,\n          11.0645],\n        [-51.7065, -52.2697,   9.8301,  -8.2097,   7.5294,  -5.0452, -13.8164,\n           1.9140],\n        [ -4.7599,   1.0423,  13.7845,  -2.2222,  20.3552, -46.8187,  22.7475,\n         -43.1153],\n        [-45.6436,  86.0580, -41.1789,   2.7945,  21.6468,  27.4330,   1.7031,\n         -12.3435],\n        [-17.2692,  36.1911,  -1.8863,  23.3779, -42.2936, -34.8993,  18.8644,\n          37.8663],\n        [-67.1890,  -1.7815,  -3.6916,   3.0762,   3.6283, -14.2628, -54.1798,\n           4.2659]]) tensor([[ 24.3418,  26.2179,  -6.8075,   7.2431,  -5.9791,  14.7296,  16.7693,\n         -12.0208],\n        [ -9.2909, -33.4937, -45.0836,   2.3782,   2.8474, -12.4469,   7.7492,\n           1.0069],\n        [ 31.4234,   7.6915,  18.3813,  -9.7053,  -0.7288,   0.1574,  -9.4108,\n           5.0289],\n        [ 33.4081,  16.6533,  30.9166, -16.0425,   2.9648,  -4.0369,   6.6590,\n           0.4690],\n        [ 11.8100,   5.4315,  69.2526, -33.6925,  16.2483,   4.5337,  11.7333,\n           7.7448],\n        [  4.1968,  -0.6881, -24.6253,  12.0952, -15.2829, -12.5102,   8.9368,\n           4.3124],\n        [ -8.8061,   0.3138,   3.6692,   1.7558,  -2.2397,  11.6223,   0.1514,\n           2.0208],\n        [-52.6169,   0.9578, -32.7799,   6.0844,   4.2431,   5.8750, -23.2917,\n         -19.4052],\n        [-13.3788,  -3.9841,  10.6965,  11.4512,   3.5704,  -5.2707,  -8.9532,\n          11.3191],\n        [-21.0874, -19.0998, -23.6198,  18.4323,  -5.6435,  -2.6534, -10.3434,\n          -0.4759]])\n","output_type":"stream"}]},{"cell_type":"code","source":"test_car_image_paths=[\n    '/kaggle/input/testcarsnoncars/testcar/test5.jpg',\n'/kaggle/input/testcarsnoncars/testcar/testcar4.jpg',\n'/kaggle/input/testcarsnoncars/testcar/testcar1.png',\n'/kaggle/input/testcarsnoncars/testcar/testcar3.jpg',\n'/kaggle/input/testcarsnoncars/testcar/testcar2.jpg'   \n]\ntest_not_car_image_paths=[\n    '/kaggle/input/testcarsnoncars/nontestcar/testnotcar5.jpg',\n'/kaggle/input/testcarsnoncars/nontestcar/testnotcar3.jpg',\n'/kaggle/input/testcarsnoncars/nontestcar/testnotcar4.jpg',\n'/kaggle/input/testcarsnoncars/nontestcar/testnotcar1.jpg',\n'/kaggle/input/testcarsnoncars/nontestcar/testnotcar2.jpg'\n]\n\n# Use the function to load car and not-car images\ntest_car_images = load_and_preprocess_images(test_car_image_paths)\ntest_not_car_images = load_and_preprocess_images(test_not_car_image_paths)\n\n# Verify the shapes of the loaded image matrices\ntest_car_images.shape, test_not_car_images.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:05.116063Z","iopub.execute_input":"2024-04-07T10:56:05.116393Z","iopub.status.idle":"2024-04-07T10:56:05.216590Z","shell.execute_reply.started":"2024-04-07T10:56:05.116366Z","shell.execute_reply":"2024-04-07T10:56:05.215663Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"((5, 12288), (5, 12288))"},"metadata":{}}]},{"cell_type":"code","source":"# Convert the new test data to PyTorch tensors\ntest_car_images_tensor = torch.tensor(test_car_images, dtype=torch.float32)\ntest_not_car_images_tensor = torch.tensor(test_not_car_images, dtype=torch.float32)\n\n# Standardize the new data using the mean and standard deviation from the training data\ntest_car_std = (test_car_images_tensor - mean_car) / std_car\ntest_not_car_std = (test_not_car_images_tensor - mean_not_car) / std_not_car\n\n# Project the new standardized data onto the PCA subspace to get PCA subspace coefficients\ntest_car_pca_car = torch.mm(test_car_std, principal_components_car)\ntest_car_pca_not_car = torch.mm(test_car_std, principal_components_not_car)\ntest_not_car_pca_not_car = torch.mm(test_not_car_std, principal_components_not_car)\ntest_not_car_pca_car = torch.mm(test_not_car_std, principal_components_car)\n\nprint(test_car_pca_car.shape,test_not_car_pca_not_car.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:05.217889Z","iopub.execute_input":"2024-04-07T10:56:05.218198Z","iopub.status.idle":"2024-04-07T10:56:05.228983Z","shell.execute_reply.started":"2024-04-07T10:56:05.218171Z","shell.execute_reply":"2024-04-07T10:56:05.228048Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"torch.Size([5, 8]) torch.Size([5, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reconstruct the test car images from the PCA coefficients\ntest_car_reconstructed_car = torch.mm(test_car_pca_car, principal_components_car.t()) * std_car + mean_car\ntest_car_reconstructed_not_car=torch.mm(test_car_pca_car,principal_components_not_car.t())*std_not_car+mean_not_car\n\n# Reconstruct the test not-car images from the PCA coefficients\ntest_not_car_reconstructed_not_car = torch.mm(test_not_car_pca_not_car, principal_components_not_car.t()) * std_not_car + mean_not_car\ntest_not_car_reconstructed_car = torch.mm(test_not_car_pca_not_car, principal_components_car.t()) * std_car + mean_car\n# Now, test_car_reconstructed and test_not_car_reconstructed are the reconstructed images\n# The shapes of the reconstructed images will match the original dimensionality of the images\ntest_car_reconstructed_car.shape, test_not_car_reconstructed_not_car.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:05.230219Z","iopub.execute_input":"2024-04-07T10:56:05.230557Z","iopub.status.idle":"2024-04-07T10:56:05.250387Z","shell.execute_reply.started":"2024-04-07T10:56:05.230530Z","shell.execute_reply":"2024-04-07T10:56:05.249221Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(torch.Size([5, 12288]), torch.Size([5, 12288]))"},"metadata":{}}]},{"cell_type":"code","source":"#original test labels\noriginal_test_labels=[\"car\",\"car\",\"car\",\"car\",\"car\",\"not car\",\"not car\",\"not car\",\"not car\",\"not car\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:20.235274Z","iopub.execute_input":"2024-04-07T10:56:20.235639Z","iopub.status.idle":"2024-04-07T10:56:20.243076Z","shell.execute_reply.started":"2024-04-07T10:56:20.235610Z","shell.execute_reply":"2024-04-07T10:56:20.241975Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#Mean squared error is the reconstruction error here\ndef reconstruction_error(original,reconstructed):\n    return ((original-reconstructed)**2).mean(dim=1)\n\nmse_test_car_car=reconstruction_error(test_car_std,test_car_reconstructed_car)\nmse_test_car_not_car=reconstruction_error(test_car_std,test_car_reconstructed_not_car)\n\nmse_test_not_car_not_car=reconstruction_error(test_not_car_std,test_not_car_reconstructed_not_car)\nmse_test_not_car_car=reconstruction_error(test_not_car_std,test_not_car_reconstructed_car)\n\n#if error from car basis is less then classify as car else classify as not car\nclassification_car=mse_test_car_car<mse_test_car_not_car\nclassification_not_car=mse_test_not_car_not_car<mse_test_not_car_car\n\nclassified_labels_car=[\"car\" if pred else \"not car\" for pred in classification_car]\nclassified_labels_not_car=[\"not car\" if pred else \"car\" for pred in classification_not_car]\n\nclassification_results_combined=classified_labels_car+classified_labels_not_car\nprint(\"Combined classification results: \",classification_results_combined)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:56:05.251871Z","iopub.execute_input":"2024-04-07T10:56:05.252209Z","iopub.status.idle":"2024-04-07T10:56:05.262676Z","shell.execute_reply.started":"2024-04-07T10:56:05.252172Z","shell.execute_reply":"2024-04-07T10:56:05.261552Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Combined classification results:  ['car', 'car', 'not car', 'car', 'car', 'not car', 'not car', 'car', 'not car', 'car']\n","output_type":"stream"}]},{"cell_type":"code","source":"original_labels_tensor = torch.tensor([1 if label == \"car\" else 0 for label in original_test_labels], dtype=torch.uint8)\npredicted_labels_tensor = torch.tensor([1 if label == \"car\" else 0 for label in classification_results_combined], dtype=torch.uint8)\nprint(original_labels_tensor)\nprint(predicted_labels_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:05:38.402485Z","iopub.execute_input":"2024-04-07T11:05:38.402895Z","iopub.status.idle":"2024-04-07T11:05:38.410450Z","shell.execute_reply.started":"2024-04-07T11:05:38.402864Z","shell.execute_reply":"2024-04-07T11:05:38.409393Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.uint8)\ntensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1], dtype=torch.uint8)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Classification accuracy calculation\n\ncorrect_predictions = torch.sum(predicted_labels_tensor == original_labels_tensor).item()\ntotal_predictions = len(original_test_labels)\naccuracy = correct_predictions / total_predictions\nprint(f\"Classification accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T11:05:57.760724Z","iopub.execute_input":"2024-04-07T11:05:57.761101Z","iopub.status.idle":"2024-04-07T11:05:57.767462Z","shell.execute_reply.started":"2024-04-07T11:05:57.761073Z","shell.execute_reply":"2024-04-07T11:05:57.766478Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Classification accuracy: 0.70\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}